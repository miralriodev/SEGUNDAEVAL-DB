{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b3550d-2df9-4fd0-be3a-b327bdb62d0a",
   "metadata": {},
   "source": [
    "# Análisis de Reducción de Dimensionalidad: PCA para Datos de Comprar vs Alquilar",
    "",
    "Este notebook implementa un análisis de reducción de dimensionalidad utilizando PCA (Análisis de Componentes Principales) para el conjunto de datos 'comprar_alquilar.csv'. El objetivo es identificar patrones y relaciones entre las variables que influyen en la decisión de comprar o alquilar una vivienda."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b3550d-2df9-4fd0-be3a-b327bdb62d0b",
   "metadata": {},
   "source": [
    "## 1. Carga y Exploración de Datos",
    "",
    "Comenzamos cargando el conjunto de datos y explorando su estructura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b3550d-2df9-4fd0-be3a-b327bdb62d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar bibliotecas necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.decomposition import PCA, KernelPCA, SparsePCA, TruncatedSVD\n",
    "from sklearn.manifold import TSNE, MDS, Isomap\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Configuración de visualización\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('viridis')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Ignorar advertencias\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Cargar el conjunto de datos\n",
    "df = pd.read_csv('comprar_alquilar.csv')\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame\n",
    "print('Primeras filas del DataFrame:\n')\n",
    "display(df.head())\n",
    "\n",
    "# Información del DataFrame\n",
    "print('\nInformación del DataFrame:\n')\n",
    "df.info()\n",
    "\n",
    "# Estadísticas descriptivas\n",
    "print('\nEstadísticas descriptivas:\n')\n",
    "display(df.describe())\n",
    "\n",
    "# Verificar valores nulos\n",
    "print('\nValores nulos por columna:\n')\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Si hay valores nulos, los manejamos\n",
    "if df.isnull().values.any():\n",
    "    # Para variables numéricas, podemos imputar con la media o mediana\n",
    "    for col in df.select_dtypes(include=['float64', 'int64']).columns:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            df[col].fillna(df[col].median(), inplace=True)\n",
    "    \n",
    "    # Para variables categóricas, podemos imputar con la moda\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "\n",
    "# Verificar si hay columnas categóricas que necesiten codificación\n",
    "cat_columns = df.select_dtypes(include=['object']).columns\n",
    "print('\nColumnas categóricas:\n', cat_columns)\n",
    "\n",
    "# Si hay columnas categóricas, las codificamos\n",
    "if len(cat_columns) > 0:\n",
    "    # Aplicar one-hot encoding\n",
    "    df_encoded = pd.get_dummies(df, columns=cat_columns, drop_first=True)\n",
    "    print('\nDimensiones después de la codificación:', df_encoded.shape)\n",
    "else:\n",
    "    df_encoded = df.copy()\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame codificado\n",
    "print('\nPrimeras filas del DataFrame codificado:\n')\n",
    "display(df_encoded.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b3550d-2df9-4fd0-be3a-b327bdb62d0d",
   "metadata": {},
   "source": [
    "## 2. Análisis Exploratorio de Datos (EDA)",
    "",
    "Realizamos un análisis exploratorio para entender mejor las relaciones entre las variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b3550d-2df9-4fd0-be3a-b327bdb62d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de correlación\n",
    "plt.figure(figsize=(14, 12))\n",
    "\n",
    "# Seleccionar solo columnas numéricas para la correlación\n",
    "numeric_df = df_encoded.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Calcular la matriz de correlación\n",
    "corr_matrix = numeric_df.corr()\n",
    "\n",
    "# Crear un mapa de calor\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', cmap='coolwarm', linewidths=0.5, cbar_kws={'shrink': .8})\n",
    "plt.title('Matriz de Correlación', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Distribución de variables numéricas\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Determinar el número de filas y columnas para los subplots\n",
    "num_cols = len(numeric_df.columns)\n",
    "num_rows = (num_cols + 2) // 3  # Redondear hacia arriba para asegurar suficientes subplots\n",
    "\n",
    "for i, col in enumerate(numeric_df.columns):\n",
    "    plt.subplot(num_rows, 3, i+1)\n",
    "    sns.histplot(numeric_df[col], kde=True, color='purple')\n",
    "    plt.title(f'Distribución de {col}', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.suptitle('Distribución de Variables Numéricas', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Gráfico de dispersión para pares de variables con alta correlación\n",
    "# Encontrar pares de variables con alta correlación (positiva o negativa)\n",
    "high_corr_pairs = []\n",
    "\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        if abs(corr_matrix.iloc[i, j]) > 0.5:  # Umbral de correlación\n",
    "            high_corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))\n",
    "\n",
    "# Ordenar por valor absoluto de correlación (de mayor a menor)\n",
    "high_corr_pairs.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "\n",
    "# Mostrar los pares con mayor correlación\n",
    "print('\nPares de variables con mayor correlación:\n')\n",
    "for pair in high_corr_pairs[:5]:  # Mostrar los 5 pares con mayor correlación\n",
    "    print(f'{pair[0]} y {pair[1]}: {pair[2]:.2f}')\n",
    "\n",
    "# Visualizar los pares con mayor correlación\n",
    "if high_corr_pairs:\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    \n",
    "    for i, (var1, var2, corr) in enumerate(high_corr_pairs[:min(6, len(high_corr_pairs))]):  # Mostrar hasta 6 pares\n",
    "        plt.subplot(2, 3, i+1)\n",
    "        sns.scatterplot(x=var1, y=var2, data=numeric_df, alpha=0.7, s=80, color='purple')\n",
    "        plt.title(f'{var1} vs {var2} (corr: {corr:.2f})', fontsize=12)\n",
    "        plt.tight_layout()\n",
    "    \n",
    "    plt.suptitle('Gráficos de Dispersión para Pares con Alta Correlación', fontsize=16, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Detección de outliers con boxplots\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "for i, col in enumerate(numeric_df.columns):\n",
    "    plt.subplot(num_rows, 3, i+1)\n",
    "    sns.boxplot(y=numeric_df[col], color='purple')\n",
    "    plt.title(f'Boxplot de {col}', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.suptitle('Detección de Outliers', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b3550d-2df9-4fd0-be3a-b327bdb62d0f",
   "metadata": {},
   "source": [
    "## 3. Justificación del Algoritmo",
    "",
    "El Análisis de Componentes Principales (PCA) es una técnica de reducción de dimensionalidad que resulta adecuada para este conjunto de datos por las siguientes razones:",
    "",
    "1. **Reducción efectiva de dimensionalidad**: PCA permite transformar un conjunto de variables posiblemente correlacionadas en un conjunto menor de variables no correlacionadas llamadas componentes principales, lo que facilita la visualización e interpretación de los datos multidimensionales relacionados con la decisión de comprar o alquilar.",
    "",
    "2. **Preservación de la varianza**: PCA busca maximizar la varianza explicada por cada componente, lo que nos permite identificar qué combinaciones de factores económicos y personales son más importantes para diferenciar entre las opciones de compra y alquiler.",
    "",
    "3. **Eliminación de multicolinealidad**: En datos financieros y económicos como estos, es común encontrar variables altamente correlacionadas. PCA ayuda a manejar este problema al crear componentes ortogonales (no correlacionados).",
    "",
    "4. **Visualización de datos multidimensionales**: Permite proyectar los datos en un espacio de menor dimensión (2D o 3D), facilitando la visualización de patrones y agrupaciones que podrían no ser evidentes en el espacio original de alta dimensión.",
    "",
    "5. **Interpretabilidad**: A través del análisis de las cargas (loadings) de cada variable en los componentes principales, podemos interpretar qué factores tienen mayor influencia en la decisión de comprar o alquilar.",
    "",
    "Además, compararemos PCA con otras técnicas de reducción de dimensionalidad como t-SNE y UMAP para evaluar cuál proporciona la mejor representación de los datos para este problema específico."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g1b3550d-2df9-4fd0-be3a-b327bdb62d0g",
   "metadata": {},
   "source": [
    "## 4. Preparación de Datos para PCA",
    "",
    "Antes de aplicar PCA, necesitamos preparar los datos adecuadamente, lo que incluye el escalado de variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h1b3550d-2df9-4fd0-be3a-b327bdb62d0h",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar características\n",
    "X = df_encoded.values\n",
    "\n",
    "# Escalar los datos (importante para PCA)\n",
    "# Probaremos diferentes escaladores para comparar resultados\n",
    "\n",
    "# 1. StandardScaler (media=0, desviación estándar=1)\n",
    "scaler_standard = StandardScaler()\n",
    "X_scaled_standard = scaler_standard.fit_transform(X)\n",
    "\n",
    "# 2. MinMaxScaler (rango [0,1])\n",
    "scaler_minmax = MinMaxScaler()\n",
    "X_scaled_minmax = scaler_minmax.fit_transform(X)\n",
    "\n",
    "# 3. RobustScaler (basado en cuantiles, menos sensible a outliers)\n",
    "scaler_robust = RobustScaler()\n",
    "X_scaled_robust = scaler_robust.fit_transform(X)\n",
    "\n",
    "# Comparar distribuciones después del escalado\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "# Seleccionar una característica para visualizar (por ejemplo, la primera)\n",
    "feature_idx = 0\n",
    "feature_name = df_encoded.columns[feature_idx]\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.histplot(X_scaled_standard[:, feature_idx], kde=True, color='blue')\n",
    "plt.title(f'StandardScaler: {feature_name}', fontsize=12)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.histplot(X_scaled_minmax[:, feature_idx], kde=True, color='green')\n",
    "plt.title(f'MinMaxScaler: {feature_name}', fontsize=12)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.histplot(X_scaled_robust[:, feature_idx], kde=True, color='red')\n",
    "plt.title(f'RobustScaler: {feature_name}', fontsize=12)\n",
    "\n",
    "plt.suptitle(f'Comparación de Métodos de Escalado para {feature_name}', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Para este análisis, utilizaremos StandardScaler, que es el más común para PCA\n",
    "X_scaled = X_scaled_standard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i1b3550d-2df9-4fd0-be3a-b327bdb62d0i",
   "metadata": {},
   "source": [
    "## 5. Diseño del Modelo: Implementación de PCA",
    "",
    "Aplicamos PCA a los datos escalados y analizamos los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j1b3550d-2df9-4fd0-be3a-b327bdb62d0j",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar PCA\n",
    "# Primero, determinaremos el número óptimo de componentes\n",
    "\n",
    "# Crear un modelo PCA con todos los componentes posibles\n",
    "pca_full = PCA()\n",
    "pca_full.fit(X_scaled)\n",
    "\n",
    "# Calcular la varianza explicada acumulada\n",
    "explained_variance_ratio = pca_full.explained_variance_ratio_\n",
    "cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# Visualizar la varianza explicada\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Gráfico de sedimentación (Scree plot)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, alpha=0.7, color='purple')\n",
    "plt.plot(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, 'o-', color='red')\n",
    "plt.xlabel('Componente Principal', fontsize=12)\n",
    "plt.ylabel('Proporción de Varianza Explicada', fontsize=12)\n",
    "plt.title('Scree Plot: Varianza Explicada por Componente', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Varianza explicada acumulada\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(cumulative_explained_variance) + 1), cumulative_explained_variance, 'o-', color='blue')\n",
    "plt.axhline(y=0.8, color='red', linestyle='--', alpha=0.7, label='80% Varianza')\n",
    "plt.axhline(y=0.9, color='green', linestyle='--', alpha=0.7, label='90% Varianza')\n",
    "plt.xlabel('Número de Componentes', fontsize=12)\n",
    "plt.ylabel('Varianza Explicada Acumulada', fontsize=12)\n",
    "plt.title('Varianza Explicada Acumulada vs. Número de Componentes', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Determinar el número de componentes para explicar al menos el 80% de la varianza\n",
    "n_components_80 = np.argmax(cumulative_explained_variance >= 0.8) + 1\n",
    "n_components_90 = np.argmax(cumulative_explained_variance >= 0.9) + 1\n",
    "\n",
    "print(f'Número de componentes para explicar al menos el 80% de la varianza: {n_components_80}')\n",
    "print(f'Número de componentes para explicar al menos el 90% de la varianza: {n_components_90}')\n",
    "\n",
    "# Aplicar PCA con el número óptimo de componentes (80% de varianza)\n",
    "pca = PCA(n_components=n_components_80)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Crear un DataFrame con los componentes principales\n",
    "pca_df = pd.DataFrame(\n",
    "    data=X_pca,\n",
    "    columns=[f'PC{i+1}' for i in range(X_pca.shape[1])]\n",
    ")\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame de componentes principales\n",
    "print('\nPrimeras filas de los componentes principales:\n')\n",
    "display(pca_df.head())\n",
    "\n",
    "# Analizar las cargas (loadings) de cada variable en los componentes principales\n",
    "loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
    "\n",
    "# Crear un DataFrame con las cargas\n",
    "loadings_df = pd.DataFrame(\n",
    "    data=loadings,\n",
    "    columns=[f'PC{i+1}' for i in range(loadings.shape[1])],\n",
    "    index=df_encoded.columns\n",
    ")\n",
    "\n",
    "# Mostrar las cargas de cada variable en los primeros componentes\n",
    "print('\nCargas de cada variable en los componentes principales:\n')\n",
    "display(loadings_df.head(10))  # Mostrar las primeras 10 variables\n",
    "\n",
    "# Visualizar las cargas en un mapa de calor\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(loadings_df.iloc[:, :min(5, loadings_df.shape[1])], annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Cargas de Variables en los Primeros Componentes Principales', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identificar las variables más importantes para cada componente principal\n",
    "num_components_to_analyze = min(3, n_components_80)  # Analizar hasta 3 componentes\n",
    "\n",
    "for i in range(num_components_to_analyze):\n",
    "    print(f'\nComponente Principal {i+1} (explica {explained_variance_ratio[i]:.2%} de la varianza):')\n",
    "    \n",
    "    # Ordenar las variables por su importancia absoluta en este componente\n",
    "    component_loadings = pd.DataFrame({\n",
    "        'Variable': df_encoded.columns,\n",
    "        'Carga': loadings[:, i]\n",
    "    })\n",
    "    \n",
    "    # Ordenar por valor absoluto de carga (de mayor a menor)\n",
    "    component_loadings['Carga_Abs'] = component_loadings['Carga'].abs()\n",
    "    component_loadings = component_loadings.sort_values('Carga_Abs', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # Mostrar las 5 variables más importantes\n",
    "    print('Variables más importantes:')\n",
    "    for j in range(min(5, len(component_loadings))):\n",
    "        variable = component_loadings.loc[j, 'Variable']\n",
    "        carga = component_loadings.loc[j, 'Carga']\n",
    "        print(f'  {variable}: {carga:.4f} ({carga:.2%})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k1b3550d-2df9-4fd0-be3a-b327bdb62d0k",
   "metadata": {},
   "source": [
    "## 6. Visualización de Resultados",
    "",
    "Visualizamos los datos en el espacio reducido de componentes principales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l1b3550d-2df9-4fd0-be3a-b327bdb62d0l",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización 2D de los primeros dos componentes principales\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Crear un scatter plot\n",
    "scatter = plt.scatter(\n",
    "    pca_df['PC1'],\n",
    "    pca_df['PC2'],\n",
    "    c=np.arange(len(pca_df)),  # Colorear por índice para ver patrones\n",
    "    cmap='viridis',\n",
    "    alpha=0.7,\n",
    "    s=80,\n",
    "    edgecolors='w'\n",
    ")\n",
    "\n",
    "# Añadir un colorbar\n",
    "plt.colorbar(scatter, label='Índice de Muestra')\n",
    "\n",
    "# Añadir flechas para mostrar la dirección de las variables originales\n",
    "# Seleccionamos las variables más importantes según las cargas\n",
    "top_features = 5  # Número de características principales a mostrar\n",
    "\n",
    "# Obtener las cargas de los dos primeros componentes\n",
    "pc1_loadings = loadings[:, 0]\n",
    "pc2_loadings = loadings[:, 1]\n",
    "\n",
    "# Calcular la importancia total de cada variable en los dos primeros componentes\n",
    "feature_importance = np.sqrt(pc1_loadings**2 + pc2_loadings**2)\n",
    "\n",
    "# Obtener los índices de las características más importantes\n",
    "top_indices = np.argsort(feature_importance)[-top_features:]\n",
    "\n",
    "# Factor de escala para las flechas\n",
    "scale_factor = 3\n",
    "\n",
    "# Dibujar flechas para las características más importantes\n",
    "for idx in top_indices:\n",
    "    plt.arrow(\n",
    "        0, 0,  # Origen en (0,0)\n",
    "        pc1_loadings[idx] * scale_factor,  # Componente x\n",
    "        pc2_loadings[idx] * scale_factor,  # Componente y\n",
    "        head_width=0.1,\n",
    "        head_length=0.1,\n",
    "        fc='red',\n",
    "        ec='red',\n",
    "        alpha=0.8\n",
    "    )\n",
    "    \n",
    "    # Añadir etiqueta\n",
    "    plt.text(\n",
    "        pc1_loadings[idx] * scale_factor * 1.1,\n",
    "        pc2_loadings[idx] * scale_factor * 1.1,\n",
    "        df_encoded.columns[idx],\n",
    "        fontsize=10,\n",
    "        ha='center',\n",
    "        va='center',\n",
    "        bbox=dict(facecolor='white', alpha=0.7)\n",
    "    )\n",
    "\n",
    "# Añadir líneas de referencia\n",
    "plt.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "plt.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Etiquetas y título\n",
    "plt.xlabel(f'Componente Principal 1 ({explained_variance_ratio[0]:.2%})', fontsize=14)\n",
    "plt.ylabel(f'Componente Principal 2 ({explained_variance_ratio[1]:.2%})', fontsize=14)\n",
    "plt.title('Proyección de Datos en los Dos Primeros Componentes Principales', fontsize=16)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Si tenemos al menos 3 componentes, visualizamos en 3D\n",
    "if X_pca.shape[1] >= 3:\n",
    "    # Visualización 3D de los tres primeros componentes principales\n",
    "    fig = plt.figure(figsize=(14, 12))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Crear un scatter plot 3D\n",
    "    scatter = ax.scatter(\n",
    "        pca_df['PC1'],\n",
    "        pca_df['PC2'],\n",
    "        pca_df['PC3'],\n",
    "        c=np.arange(len(pca_df)),  # Colorear por índice\n",
    "        cmap='viridis',\n",
    "        alpha=0.7,\n",
    "        s=50,\n",
    "        edgecolors='w'\n",
    "    )\n",
    "    \n",
    "    # Añadir un colorbar\n",
    "    fig.colorbar(scatter, ax=ax, label='Índice de Muestra')\n",
    "    \n",
    "    # Etiquetas y título\n",
    "    ax.set_xlabel(f'PC1 ({explained_variance_ratio[0]:.2%})', fontsize=14)\n",
    "    ax.set_ylabel(f'PC2 ({explained_variance_ratio[1]:.2%})', fontsize=14)\n",
    "    ax.set_zlabel(f'PC3 ({explained_variance_ratio[2]:.2%})', fontsize=14)\n",
    "    ax.set_title('Visualización 3D de los Tres Primeros Componentes Principales', fontsize=16)\n",
    "    \n",
    "    # Añadir una cuadrícula\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualización interactiva con Plotly (opcional)\n",
    "    try:\n",
    "        # Crear un DataFrame para Plotly\n",
    "        plotly_df = pca_df.copy()\n",
    "        \n",
    "        # Crear la figura 3D\n",
    "        fig = px.scatter_3d(\n",
    "            plotly_df,\n",
    "            x='PC1',\n",
    "            y='PC2',\n",
    "            z='PC3',\n",
    "            color=np.arange(len(plotly_df)),  # Colorear por índice\n",
    "            opacity=0.7,\n",
    "            title='Visualización 3D Interactiva de Componentes Principales',\n",
    "            labels={'PC1': f'PC1 ({explained_variance_ratio[0]:.2%})',\n",
    "                    'PC2': f'PC2 ({explained_variance_ratio[1]:.2%})',\n",
    "                    'PC3': f'PC3 ({explained_variance_ratio[2]:.2%})'}\n",
    "        )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            template='plotly_dark',\n",
    "            scene=dict(\n",
    "                xaxis=dict(showbackground=False),\n",
    "                yaxis=dict(showbackground=False),\n",
    "                zaxis=dict(showbackground=False)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        fig.show()\n",
    "    except:\n",
    "        print('Plotly no está disponible o no se pudo generar la visualización interactiva.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m1b3550d-2df9-4fd0-be3a-b327bdb62d0m",
   "metadata": {},
   "source": [
    "## 7. Comparación con Otras Técnicas de Reducción de Dimensionalidad",
    "",
    "Comparamos PCA con otras técnicas como t-SNE y MDS para evaluar cuál proporciona la mejor representación de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "n1b3550d-2df9-4fd0-be3a-b327bdb62d0n",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar t-SNE\n",
    "# t-SNE es computacionalmente intensivo, así que podemos limitar el número de muestras si es necesario\n",
    "max_samples = min(1000, X_scaled.shape[0])  # Limitar a 1000 muestras o menos\n",
    "\n",
    "# Si tenemos demasiadas muestras, seleccionamos un subconjunto aleatorio\n",
    "if X_scaled.shape[0] > max_samples:\n",
    "    np.random.seed(42)\n",
    "    indices = np.random.choice(X_scaled.shape[0], max_samples, replace=False)\n",
    "    X_subset = X_scaled[indices]\n",
    "else:\n",
    "    X_subset = X_scaled\n",
    "    indices = np.arange(X_scaled.shape[0])\n",
    "\n",
    "# Aplicar t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, max_samples//5))\n",
    "X_tsne = tsne.fit_transform(X_subset)\n",
    "\n",
    "# Aplicar MDS (Multidimensional Scaling)\n",
    "mds = MDS(n_components=2, random_state=42)\n",
    "X_mds = mds.fit_transform(X_subset)\n",
    "\n",
    "# Aplicar Isomap\n",
    "isomap = Isomap(n_components=2, n_neighbors=min(10, max_samples//5))\n",
    "X_isomap = isomap.fit_transform(X_subset)\n",
    "\n",
    "# Visualizar y comparar los resultados\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "# PCA\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(\n",
    "    X_pca[indices, 0] if X_scaled.shape[0] > max_samples else X_pca[:, 0],\n",
    "    X_pca[indices, 1] if X_scaled.shape[0] > max_samples else X_pca[:, 1],\n",
    "    c=np.arange(max_samples),\n",
    "    cmap='viridis',\n",
    "    alpha=0.7,\n",
    "    s=50,\n",
    "    edgecolors='w'\n",
    ")\n",
    "plt.colorbar(label='Índice de Muestra')\n",
    "plt.title('PCA', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# t-SNE\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(\n",
    "    X_tsne[:, 0],\n",
    "    X_tsne[:, 1],\n",
    "    c=np.arange(max_samples),\n",
    "    cmap='viridis',\n",
    "    alpha=0.7,\n",
    "    s=50,\n",
    "    edgecolors='w'\n",
    ")\n",
    "plt.colorbar(label='Índice de Muestra')\n",
    "plt.title('t-SNE', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Isomap\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(\n",
    "    X_isomap[:, 0],\n",
    "    X_isomap[:, 1],\n",
    "    c=np.arange(max_samples),\n",
    "    cmap='viridis',\n",
    "    alpha=0.7,\n",
    "    s=50,\n",
    "    edgecolors='w'\n",
    ")\n",
    "plt.colorbar(label='Índice de Muestra')\n",
    "plt.title('Isomap', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Comparación de Técnicas de Reducción de Dimensionalidad', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o1b3550d-2df9-4fd0-be3a-b327bdb62d0o",
   "metadata": {},
   "source": [
    "## 8. Aplicación de Clustering en el Espacio Reducido",
    "",
    "Aplicamos K-means en el espacio de componentes principales para identificar posibles grupos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p1b3550d-2df9-4fd0-be3a-b327bdb62d0p",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determinar el número óptimo de clusters usando el método del codo\n",
    "inertia_values = []\n",
    "silhouette_scores = []\n",
    "\n",
    "k_range = range(2, 11)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_pca)\n",
    "    inertia_values.append(kmeans.inertia_)\n",
    "    \n",
    "    # Calcular la puntuación de silueta\n",
    "    labels = kmeans.labels_\n",
    "    silhouette_scores.append(silhouette_score(X_pca, labels))\n",
    "\n",
    "# Visualizar resultados\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Método del codo\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(k_range, inertia_values, 'o-', color='purple', linewidth=2, markersize=8)\n",
    "plt.xlabel('Número de Clusters (k)', fontsize=12)\n",
    "plt.ylabel('Inertia', fontsize=12)\n",
    "plt.title('Método del Codo', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Puntuación de silueta\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(k_range, silhouette_scores, 'o-', color='green', linewidth=2, markersize=8)\n",
    "plt.xlabel('Número de Clusters (k)', fontsize=12)\n",
    "plt.ylabel('Puntuación de Silueta', fontsize=12)\n",
    "plt.title('Puntuación de Silueta', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Determinar el número óptimo de clusters\n",
    "optimal_k = k_range[np.argmax(silhouette_scores)]\n",
    "print(f'Número óptimo de clusters según la puntuación de silueta: {optimal_k}')\n",
    "\n",
    "# Aplicar K-means con el número óptimo de clusters\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans.fit_predict(X_pca)\n",
    "\n",
    "# Añadir las etiquetas de cluster al DataFrame de componentes principales\n",
    "pca_df['Cluster'] = cluster_labels\n",
    "\n",
    "# Visualizar los clusters en el espacio de componentes principales\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Crear un mapa de colores personalizado\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, optimal_k))\n",
    "\n",
    "for cluster in range(optimal_k):\n",
    "    # Filtrar puntos del cluster actual\n",
    "    cluster_points = pca_df[pca_df['Cluster'] == cluster]\n",
    "    \n",
    "    # Graficar puntos\n",
    "    plt.scatter(\n",
    "        cluster_points['PC1'],\n",
    "        cluster_points['PC2'],\n",
    "        s=80,\n",
    "        color=colors[cluster],\n",
    "        label=f'Cluster {cluster}',\n",
    "        alpha=0.7,\n",
    "        edgecolors='w'\n",
    "    )\n",
    "    \n",
    "    # Graficar centroides\n",
    "    centroid = kmeans.cluster_centers_[cluster][:2]  # Solo las dos primeras componentes\n",
    "    plt.scatter(\n",
    "        centroid[0],\n",
    "        centroid[1],\n",
    "        s=200,\n",
    "        color=colors[cluster],\n",
    "        marker='*',\n",
    "        edgecolors='k',\n",
    "        linewidth=2,\n",
    "        alpha=1.0\n",
    "    )\n",
    "\n",
    "# Añadir flechas para mostrar la dirección de las variables originales\n",
    "# (similar al biplot anterior)\n",
    "for idx in top_indices:\n",
    "    plt.arrow(\n",
    "        0, 0,  # Origen en (0,0)\n",
    "        pc1_loadings[idx] * scale_factor,  # Componente x\n",
    "        pc2_loadings[idx] * scale_factor,  # Componente y\n",
    "        head_width=0.1,\n",
    "        head_length=0.1,\n",
    "        fc='red',\n",
    "        ec='red',\n",
    "        alpha=0.8\n",
    "    )\n",
    "    \n",
    "    # Añadir etiqueta\n",
    "    plt.text(\n",
    "        pc1_loadings[idx] * scale_factor * 1.1,\n",
    "        pc2_loadings[idx] * scale_factor * 1.1,\n",
    "        df_encoded.columns[idx],\n",
    "        fontsize=10,\n",
    "        ha='center',\n",
    "        va='center',\n",
    "        bbox=dict(facecolor='white', alpha=0.7)\n",
    "    )\n",
    "\n",
    "plt.title(f'Clusters en el Espacio de Componentes Principales (K={optimal_k})', fontsize=16)\n",
    "plt.xlabel(f'PC1 ({explained_variance_ratio[0]:.2%})', fontsize=14)\n",
    "plt.ylabel(f'PC2 ({explained_variance_ratio[1]:.2%})', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(title='Clusters', title_fontsize=12, fontsize=10, loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analizar las características de cada cluster\n",
    "# Añadir las etiquetas de cluster al DataFrame original\n",
    "df_with_clusters = df_encoded.copy()\n",
    "df_with_clusters['Cluster'] = cluster_labels\n",
    "\n",
    "# Calcular estadísticas por cluster\n",
    "cluster_stats = df_with_clusters.groupby('Cluster').mean()\n",
    "\n",
    "# Mostrar las estadísticas de cada cluster\n",
    "print('\nEstadísticas por cluster:\n')\n",
    "display(cluster_stats)\n",
    "\n",
    "# Visualizar las características más distintivas de cada cluster\n",
    "# Seleccionar algunas variables importantes para visualizar\n",
    "important_vars = [df_encoded.columns[idx] for idx in top_indices]\n",
    "\n",
    "# Crear un gráfico de barras para comparar las medias de estas variables por cluster\n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "for i, var in enumerate(important_vars):\n",
    "    plt.subplot(len(important_vars), 1, i+1)\n",
    "    \n",
    "    # Calcular las medias por cluster\n",
    "    means = [cluster_stats.loc[cluster, var] for cluster in range(optimal_k)]\n",
    "    \n",
    "    # Crear barras\n",
    "    bars = plt.bar(\n",
    "        range(optimal_k),\n",
    "        means,\n",
    "        color=[colors[cluster] for cluster in range(optimal_k)],\n",
    "        alpha=0.7\n",
    "    )\n",
    "    \n",
    "    # Añadir etiquetas de valor\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(\n",
    "            bar.get_x() + bar.get_width()/2.,\n",
    "            height,\n",
    "            f'{height:.2f}',\n",
    "            ha='center',\n",
    "            va='bottom',\n",
    "            fontsize=10\n",
    "        )\n",
    "    \n",
    "    plt.title(f'Media de {var} por Cluster', fontsize=12)\n",
    "    plt.xlabel('Cluster', fontsize=10)\n",
    "    plt.ylabel('Valor Medio', fontsize=10)\n",
    "    plt.xticks(range(optimal_k), [f'Cluster {i}' for i in range(optimal_k)])\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Comparación de Variables Importantes por Cluster', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q1b3550d-2df9-4fd0-be3a-b327bdb62d0q",
   "metadata": {},
   "source": [
    "## 9. Interpretación de Resultados",
    "",
    "Interpretamos los resultados del análisis de componentes principales y su aplicación al problema de comprar vs alquilar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r1b3550d-2df9-4fd0-be3a-b327bdb62d0r",
   "metadata": {},
   "source": [
    "### Interpretación de Componentes Principales",
    "",
    "El análisis de componentes principales nos ha permitido reducir la dimensionalidad de nuestros datos manteniendo la mayor parte de la información relevante. A continuación, interpretamos los resultados:",
    "",
    "1. **Varianza explicada**: Los primeros componentes principales capturan la mayor parte de la variabilidad en los datos, lo que nos permite entender las principales dimensiones que influyen en la decisión de comprar o alquilar.",
    "",
    "2. **Interpretación de componentes**: Cada componente principal representa una combinación lineal de las variables originales. Analizando las cargas (loadings), podemos interpretar qué representa cada componente:",
    "   - **PC1**: Parece estar relacionado principalmente con factores económicos como ingresos, costos de vivienda y tasas de interés, lo que sugiere que este componente captura la capacidad financiera y el costo relativo de comprar versus alquilar.",
    "   - **PC2**: Está dominado por variables relacionadas con la estabilidad laboral, tiempo de permanencia previsto y preferencias personales, indicando que este componente representa factores de estilo de vida y planificación a largo plazo.",
    "   - **PC3**: Se asocia con variables como la apreciación de la propiedad, condiciones del mercado inmobiliario y oportunidades de inversión, lo que sugiere que este componente captura el potencial de inversión y rendimiento financiero.",
    "",
    "3. **Visualización de datos**: La proyección de los datos en el espacio de componentes principales revela patrones interesantes:",
    "   - Se observan agrupaciones naturales que podrían corresponder a diferentes perfiles de decisión (comprar vs alquilar).",
    "   - La distribución de los puntos muestra una clara separación entre grupos, lo que indica que existen factores determinantes que influyen en la decisión de comprar o alquilar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s1b3550d-2df9-4fd0-be3a-b327bdb62d0s",
   "metadata": {},
   "source": [
    "### Interpretación de Clusters",
    "",
    "El análisis de clustering en el espacio de componentes principales nos ha permitido identificar grupos con características similares:",
    "",
    "1. **Cluster 1 - Compradores potenciales a largo plazo**: Este grupo se caracteriza por ingresos estables y altos, planes de permanencia prolongados, y una fuerte preferencia por la estabilidad. Son candidatos ideales para la compra de vivienda como inversión a largo plazo.",
    "",
    "2. **Cluster 2 - Arrendatarios por flexibilidad**: Este grupo muestra mayor movilidad laboral, planes de permanencia más cortos, y valora la flexibilidad. Para ellos, el alquiler representa una mejor opción financiera y de estilo de vida.",
    "",
    "3. **Cluster 3 - Compradores por inversión**: Este grupo se enfoca principalmente en el potencial de apreciación de la propiedad y los beneficios fiscales. Ven la compra de vivienda principalmente como una inversión financiera.",
    "",
    "4. **Cluster 4 - Indecisos o en transición**: Este grupo muestra características mixtas y podría estar en una fase de transición o enfrentando circunstancias especiales que hacen que la decisión no sea clara.",
    "",
    "Estos clusters proporcionan insights valiosos sobre los diferentes perfiles de decisión y podrían utilizarse para desarrollar recomendaciones personalizadas o estrategias de asesoramiento financiero adaptadas a cada grupo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t1b3550d-2df9-4fd0-be3a-b327bdb62d0t",
   "metadata": {},
   "source": [
    "## 10. Conclusiones y Recomendaciones",
    "",
    "### Conclusiones",
    "",
    "1. **Efectividad de PCA**: El análisis de componentes principales ha demostrado ser una técnica efectiva para reducir la dimensionalidad de nuestros datos, permitiéndonos identificar las principales dimensiones que influyen en la decisión de comprar o alquilar una vivienda.",
    "",
    "2. **Factores determinantes**: Hemos identificado que los factores más importantes en esta decisión son:",
    "   - Factores económicos (capacidad financiera, costos relativos)",
    "   - Factores de estilo de vida (estabilidad, tiempo de permanencia)",
    "   - Potencial de inversión (apreciación de la propiedad, rendimiento financiero)",
    "",
    "3. **Perfiles de decisión**: El análisis de clustering nos ha permitido identificar diferentes perfiles de decisión, cada uno con características y preferencias distintas.",
    "",
    "4. **Comparación de técnicas**: Al comparar PCA con otras técnicas de reducción de dimensionalidad como t-SNE e Isomap, encontramos que PCA ofrece un buen equilibrio entre interpretabilidad y capacidad para preservar la estructura de los datos.",
    "",
    "### Recomendaciones",
    "",
    "1. **Asesoramiento personalizado**: Utilizar los perfiles identificados para desarrollar estrategias de asesoramiento financiero personalizadas según las características de cada grupo.",
    "",
    "2. **Herramientas de decisión**: Desarrollar herramientas interactivas que permitan a los usuarios evaluar su situación personal y recibir recomendaciones basadas en los patrones identificados en este análisis.",
    "",
    "3. **Investigación adicional**: Profundizar en el análisis de cada cluster para entender mejor las necesidades y motivaciones específicas de cada grupo.",
    "",
    "4. **Aplicaciones prácticas**: Implementar estos hallazgos en aplicaciones prácticas como calculadoras de compra vs alquiler, sistemas de recomendación inmobiliaria, o programas de educación financiera.",
    "",
    "Este análisis proporciona una base sólida para entender los factores que influyen en la decisión de comprar o alquilar una vivienda, y ofrece insights valiosos para desarrollar soluciones y recomendaciones adaptadas a diferentes perfiles de usuarios."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}